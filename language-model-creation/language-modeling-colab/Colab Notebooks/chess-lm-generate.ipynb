{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chess-lm-generate.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PX19Ree5UBZP"},"source":["# Create single Language Model + Scorer for Voice-Chess\n","\n","Coqui v1.3.0 + KenLM latest / 5-gram\n","\n","This notebook does not require GPU.\n","\n","Default directory structure on Google Drive (pre-generated):\n","```\n","/voice-chess        # application\n","  /lm-raw-text      # raw command text generated\n","  /checkpoints      # checkpoints for languages (produced by previous trainings)\n","    /en             # each in own directory\n","    /tr\n","    ...\n","  /lm               # Language models and scorers generated\n","  /models           # Acoustic models collected (not used here, but needed for inference in application)\n","```\n","\n","**INPUT:**\n","\n","1. Put your localized raw chess commands in /voice-chess/lm-raw-text directory as <languagecode>.txt file (e.g. en.txt)\n","2. You alphabet for the model you will use should be under /voice-chess/checkpoints/<languagecode> directory (e.g. /voice-chess/checkpoints/en/* ).\n","\n","**OUTPUT:**\n","\n","3 result files can be found in /voice-chess/lm directory. Existing files will be overwritten. From these, *.scorer file will be used in the application.\n","*.txt files will include all tokens\n","*.binary file is your language model"]},{"cell_type":"markdown","source":["<H2>SPECIFY LANGUAGE CODE</H2>"],"metadata":{"id":"dDT06U33nmlP"}},{"cell_type":"code","source":["LANGUAGECODE = \"tr\""],"metadata":{"id":"wXyZijgcnviT","executionInfo":{"status":"ok","timestamp":1657100311187,"user_tz":-180,"elapsed":6,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Other Constants, adapt if needed\n","COQUI=\"1.3.0\"                                     # Coqui STT version used\n","DRIVEPATH=\"/content/drive/MyDrive/voice-chess\"    # Where you keep your work on Google Drive\n","LOCALPATH=\"/content/data/lm\"                      # A local working directory in Colab\n","TEXTDIR=\"lm-raw-text\"                             # Subdirectory names\n","LMDIR=\"lm\"\n","CHECKPOINSTDIR=\"checkpoints\""],"metadata":{"id":"S2kdGoFy28IB","executionInfo":{"status":"ok","timestamp":1657100311187,"user_tz":-180,"elapsed":5,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fMRwVKFCJmtm"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"L6MGhhWLB56b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657100311188,"user_tz":-180,"elapsed":6,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"3f3b91fb-1e4a-4b08-fa43-2bc3ac0ab151"},"source":["# Switch back to v1 - See: https://colab.research.google.com/notebooks/tensorflow_version.ipynb#scrollTo=NeWVBhf1VxlH\n","%tensorflow_version 1.x"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"id":"V4b9iJI7JZ6e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657100331561,"user_tz":-180,"elapsed":20377,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"7e99ce7c-778b-4bbe-814c-00b91bd93233"},"source":["# mount your private google drive\n","from google.colab import drive\n","import shutil\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"NAMwVOvtJvZj"},"source":["## Basic Setup"]},{"cell_type":"code","metadata":{"id":"URSGwY5qr3gQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657100393842,"user_tz":-180,"elapsed":62288,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"d4183ba9-757e-427d-9e7d-1dceeacfe443"},"source":["# Install Coqui STT \n","!git clone --depth 1 --branch v{COQUI} https://github.com/coqui-ai/STT.git\n","!cd STT; pip install -U pip wheel setuptools; pip install ."],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'STT'...\n","remote: Enumerating objects: 2202, done.\u001b[K\n","remote: Counting objects: 100% (2202/2202), done.\u001b[K\n","remote: Compressing objects: 100% (1400/1400), done.\u001b[K\n","remote: Total 2202 (delta 822), reused 1829 (delta 712), pack-reused 0\u001b[K\n","Receiving objects: 100% (2202/2202), 12.99 MiB | 31.97 MiB/s, done.\n","Resolving deltas: 100% (822/822), done.\n","Note: checking out '148fa74387a2082555dabd243193c6ca8cb19016'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by performing another checkout.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -b with the checkout command again. Example:\n","\n","  git checkout -b <new-branch-name>\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 26.7 MB/s \n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n","Collecting setuptools\n","  Downloading setuptools-63.1.0-py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 61.2 MB/s \n","\u001b[?25hInstalling collected packages: setuptools, pip\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed pip-22.1.2 setuptools-63.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/STT\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting attrdict\n","  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (0.0.1)\n","Collecting coqpit\n","  Downloading coqpit-0.0.16-py3-none-any.whl (13 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (1.21.6)\n","Collecting optuna\n","  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numba<=0.53.1 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (0.51.2)\n","Collecting opuslib==2.0.0\n","  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (1.3.5)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (3.38.0)\n","Collecting pyogg>=0.6.14a1\n","  Downloading PyOgg-0.6.14a1.tar.gz (35 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (0.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (2.23.0)\n","Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (2.13.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (1.15.0)\n","Collecting sox\n","  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (0.10.3.post1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (4.64.0)\n","Collecting webdataset\n","  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting miniaudio\n","  Downloading miniaudio-1.51-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.5/588.5 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coqui_stt_ctcdecoder==1.3.0\n","  Downloading coqui_stt_ctcdecoder-1.3.0-cp37-cp37m-manylinux_2_24_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow==1.15.4\n","  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (0.37.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.1.2)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.15.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.46.3)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (0.2.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.14.1)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.15.0)\n","Collecting numpy\n","  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui-stt-training==1.3.0) (63.1.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui-stt-training==1.3.0) (0.34.0)\n","Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->coqui-stt-training==1.3.0) (1.4.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->coqui-stt-training==1.3.0) (4.6.3)\n","Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from miniaudio->coqui-stt-training==1.3.0) (1.15.0)\n","Collecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui-stt-training==1.3.0) (21.3)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui-stt-training==1.3.0) (1.4.37)\n","Collecting alembic\n","  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.3/209.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna->coqui-stt-training==1.3.0) (3.13)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui-stt-training==1.3.0) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui-stt-training==1.3.0) (2022.1)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->coqui-stt-training==1.3.0) (3.3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.3.0) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.3.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.3.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.3.0) (3.0.4)\n","Collecting braceexpand\n","  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12.0->miniaudio->coqui-stt-training==1.3.0) (2.21)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->coqui-stt-training==1.3.0) (3.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna->coqui-stt-training==1.3.0) (3.0.9)\n","Requirement already satisfied: greenlet!=0.4.17 in /tensorflow-1.15.2/python3.7 (from sqlalchemy>=1.1.0->optuna->coqui-stt-training==1.3.0) (0.4.15)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->coqui-stt-training==1.3.0) (4.11.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui-stt-training==1.3.0) (3.3.7)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.0.1)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->coqui-stt-training==1.3.0) (5.7.1)\n","Collecting Mako\n","  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui-stt-training==1.3.0) (3.3.0)\n","Collecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Collecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui-stt-training==1.3.0) (0.2.5)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui-stt-training==1.3.0) (4.1.1)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui-stt-training==1.3.0) (21.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna->coqui-stt-training==1.3.0) (3.8.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.5.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna->coqui-stt-training==1.3.0) (2.0.1)\n","Building wheels for collected packages: coqui-stt-training, opuslib, gast, pyogg, pyperclip\n","  Building wheel for coqui-stt-training (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for coqui-stt-training: filename=coqui_stt_training-1.3.0-py3-none-any.whl size=87226 sha256=a4a535a7627fa0b94bb7e9034113057a1f401b509da381d66a01c3aed8e6d84e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-x3tuvgql/wheels/59/0f/67/991d0aff52677d5260e383afececfae694d28b02bfeaacf4ed\n","  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=10988 sha256=a53d444a8d891193b8194a94f4d7823b35a02ec433aa4524797dc136d4d3cec6\n","  Stored in directory: /root/.cache/pip/wheels/e5/ba/d4/0e81231a9797fbb262ae3a54fd761fab850db7f32d94a3283a\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=36c7e4c309f5a3df32277279b59bedad3387d4539edb97f18c6fce3027b946c3\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","  Building wheel for pyogg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyogg: filename=PyOgg-0.6.14a1-py2.py3-none-any.whl size=35311 sha256=b3eaac9bb62f986ee36a319efe973439d35b40ee13b9db882ac7067016ed307a\n","  Stored in directory: /root/.cache/pip/wheels/45/aa/6f/5fb54a0a14846e4202945a65937c7f3eb245af5031a141147a\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11123 sha256=4b9f79619129d0b7a95d25060162408ccd62f8f6e1ad20e39d94460bf6580b07\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built coqui-stt-training opuslib gast pyogg pyperclip\n","Installing collected packages: pyperclip, pyogg, opuslib, braceexpand, pbr, numpy, gast, coqpit, colorlog, autopage, attrdict, webdataset, stevedore, sox, miniaudio, Mako, coqui_stt_ctcdecoder, cmd2, cmaes, cliff, alembic, tensorflow, optuna, coqui-stt-training\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 1.15.2\n","    Uninstalling tensorflow-1.15.2:\n","      Successfully uninstalled tensorflow-1.15.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lucid 0.3.10 requires umap-learn, which is not installed.\n","xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.4 which is incompatible.\n","jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.8 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Mako-1.2.1 alembic-1.8.0 attrdict-2.0.1 autopage-0.5.1 braceexpand-0.1.7 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 coqpit-0.0.16 coqui-stt-training-1.3.0 coqui_stt_ctcdecoder-1.3.0 gast-0.2.2 miniaudio-1.51 numpy-1.18.5 optuna-2.10.1 opuslib-2.0.0 pbr-5.9.0 pyogg-0.6.14a1 pyperclip-1.8.2 sox-1.4.1 stevedore-3.5.0 tensorflow-2.8.2+zzzcolab20220527125636 webdataset-0.2.5\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# Tensorflow GPU\n","# Needed if you want to run evaluate to test / voice & text corpus needed for that, so we leave it out\n","#!pip install tensorflow-gpu==1.15.4"],"metadata":{"id":"K4wBlgf25QyN","executionInfo":{"status":"ok","timestamp":1657100393842,"user_tz":-180,"elapsed":6,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"FLp19SjmFBLj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657100480547,"user_tz":-180,"elapsed":86710,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"33022dab-f8b1-42ef-bf95-3e7f1042614d"},"source":["# Get KenLM\n","!git clone https://github.com/kpu/kenlm.git && cd kenlm && mkdir build && cd build/ && cmake .. && make -j 4"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'kenlm'...\n","remote: Enumerating objects: 14102, done.\u001b[K\n","remote: Counting objects:   0% (1/415)\u001b[K\rremote: Counting objects:   1% (5/415)\u001b[K\rremote: Counting objects:   2% (9/415)\u001b[K\rremote: Counting objects:   3% (13/415)\u001b[K\rremote: Counting objects:   4% (17/415)\u001b[K\rremote: Counting objects:   5% (21/415)\u001b[K\rremote: Counting objects:   6% (25/415)\u001b[K\rremote: Counting objects:   7% (30/415)\u001b[K\rremote: Counting objects:   8% (34/415)\u001b[K\rremote: Counting objects:   9% (38/415)\u001b[K\rremote: Counting objects:  10% (42/415)\u001b[K\rremote: Counting objects:  11% (46/415)\u001b[K\rremote: Counting objects:  12% (50/415)\u001b[K\rremote: Counting objects:  13% (54/415)\u001b[K\rremote: Counting objects:  14% (59/415)\u001b[K\rremote: Counting objects:  15% (63/415)\u001b[K\rremote: Counting objects:  16% (67/415)\u001b[K\rremote: Counting objects:  17% (71/415)\u001b[K\rremote: Counting objects:  18% (75/415)\u001b[K\rremote: Counting objects:  19% (79/415)\u001b[K\rremote: Counting objects:  20% (83/415)\u001b[K\rremote: Counting objects:  21% (88/415)\u001b[K\rremote: Counting objects:  22% (92/415)\u001b[K\rremote: Counting objects:  23% (96/415)\u001b[K\rremote: Counting objects:  24% (100/415)\u001b[K\rremote: Counting objects:  25% (104/415)\u001b[K\rremote: Counting objects:  26% (108/415)\u001b[K\rremote: Counting objects:  27% (113/415)\u001b[K\rremote: Counting objects:  28% (117/415)\u001b[K\rremote: Counting objects:  29% (121/415)\u001b[K\rremote: Counting objects:  30% (125/415)\u001b[K\rremote: Counting objects:  31% (129/415)\u001b[K\rremote: Counting objects:  32% (133/415)\u001b[K\rremote: Counting objects:  33% (137/415)\u001b[K\rremote: Counting objects:  34% (142/415)\u001b[K\rremote: Counting objects:  35% (146/415)\u001b[K\rremote: Counting objects:  36% (150/415)\u001b[K\rremote: Counting objects:  37% (154/415)\u001b[K\rremote: Counting objects:  38% (158/415)\u001b[K\rremote: Counting objects:  39% (162/415)\u001b[K\rremote: Counting objects:  40% (166/415)\u001b[K\rremote: Counting objects:  41% (171/415)\u001b[K\rremote: Counting objects:  42% (175/415)\u001b[K\rremote: Counting objects:  43% (179/415)\u001b[K\rremote: Counting objects:  44% (183/415)\u001b[K\rremote: Counting objects:  45% (187/415)\u001b[K\rremote: Counting objects:  46% (191/415)\u001b[K\rremote: Counting objects:  47% (196/415)\u001b[K\rremote: Counting objects:  48% (200/415)\u001b[K\rremote: Counting objects:  49% (204/415)\u001b[K\rremote: Counting objects:  50% (208/415)\u001b[K\rremote: Counting objects:  51% (212/415)\u001b[K\rremote: Counting objects:  52% (216/415)\u001b[K\rremote: Counting objects:  53% (220/415)\u001b[K\rremote: Counting objects:  54% (225/415)\u001b[K\rremote: Counting objects:  55% (229/415)\u001b[K\rremote: Counting objects:  56% (233/415)\u001b[K\rremote: Counting objects:  57% (237/415)\u001b[K\rremote: Counting objects:  58% (241/415)\u001b[K\rremote: Counting objects:  59% (245/415)\u001b[K\rremote: Counting objects:  60% (249/415)\u001b[K\rremote: Counting objects:  61% (254/415)\u001b[K\rremote: Counting objects:  62% (258/415)\u001b[K\rremote: Counting objects:  63% (262/415)\u001b[K\rremote: Counting objects:  64% (266/415)\u001b[K\rremote: Counting objects:  65% (270/415)\u001b[K\rremote: Counting objects:  66% (274/415)\u001b[K\rremote: Counting objects:  67% (279/415)\u001b[K\rremote: Counting objects:  68% (283/415)\u001b[K\rremote: Counting objects:  69% (287/415)\u001b[K\rremote: Counting objects:  70% (291/415)\u001b[K\rremote: Counting objects:  71% (295/415)\u001b[K\rremote: Counting objects:  72% (299/415)\u001b[K\rremote: Counting objects:  73% (303/415)\u001b[K\rremote: Counting objects:  74% (308/415)\u001b[K\rremote: Counting objects:  75% (312/415)\u001b[K\rremote: Counting objects:  76% (316/415)\u001b[K\rremote: Counting objects:  77% (320/415)\u001b[K\rremote: Counting objects:  78% (324/415)\u001b[K\rremote: Counting objects:  79% (328/415)\u001b[K\rremote: Counting objects:  80% (332/415)\u001b[K\rremote: Counting objects:  81% (337/415)\u001b[K\rremote: Counting objects:  82% (341/415)\u001b[K\rremote: Counting objects:  83% (345/415)\u001b[K\rremote: Counting objects:  84% (349/415)\u001b[K\rremote: Counting objects:  85% (353/415)\u001b[K\rremote: Counting objects:  86% (357/415)\u001b[K\rremote: Counting objects:  87% (362/415)\u001b[K\rremote: Counting objects:  88% (366/415)\u001b[K\rremote: Counting objects:  89% (370/415)\u001b[K\rremote: Counting objects:  90% (374/415)\u001b[K\rremote: Counting objects:  91% (378/415)\u001b[K\rremote: Counting objects:  92% (382/415)\u001b[K\rremote: Counting objects:  93% (386/415)\u001b[K\rremote: Counting objects:  94% (391/415)\u001b[K\rremote: Counting objects:  95% (395/415)\u001b[K\rremote: Counting objects:  96% (399/415)\u001b[K\rremote: Counting objects:  97% (403/415)\u001b[K\rremote: Counting objects:  98% (407/415)\u001b[K\rremote: Counting objects:  99% (411/415)\u001b[K\rremote: Counting objects: 100% (415/415)\u001b[K\rremote: Counting objects: 100% (415/415), done.\u001b[K\n","remote: Compressing objects: 100% (290/290), done.\u001b[K\n","remote: Total 14102 (delta 126), reused 371 (delta 111), pack-reused 13687\u001b[K\n","Receiving objects: 100% (14102/14102), 5.89 MiB | 19.16 MiB/s, done.\n","Resolving deltas: 100% (8006/8006), done.\n","-- The C compiler identification is GNU 7.5.0\n","-- The CXX compiler identification is GNU 7.5.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n","-- Looking for pthread.h\n","-- Looking for pthread.h - found\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n","-- Looking for pthread_create in pthreads\n","-- Looking for pthread_create in pthreads - not found\n","-- Looking for pthread_create in pthread\n","-- Looking for pthread_create in pthread - found\n","-- Found Threads: TRUE  \n","-- Found Boost: /usr/include (found suitable version \"1.65.1\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework chrono date_time atomic \n","-- Check if compiler accepts -pthread\n","-- Check if compiler accepts -pthread - yes\n","-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n","-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n","-- Looking for BZ2_bzCompressInit\n","-- Looking for BZ2_bzCompressInit - found\n","-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n","-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n","-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n","-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n","-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n","-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n","-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.2\") \n","-- Looking for clock_gettime in rt\n","-- Looking for clock_gettime in rt - found\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/kenlm/build\n","[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n","[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n","[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n","[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-to-string.cc.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/string-to-double.cc.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n","[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n","[ 38%] Built target kenlm_util\n","[ 40%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n","[ 53%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n","[ 53%] Built target kenlm_filter\n","[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n","[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n","[ 62%] Built target probing_hash_table_benchmark\n","[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n","[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n","[ 71%] Built target kenlm\n","[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n","[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n","[ 78%] Built target fragment\n","[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n","[ 80%] Built target build_binary\n","[ 81%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n","[ 82%] Built target query\n","[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n","[ 86%] Built target phrase_table_vocab\n","[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n","[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n","[ 92%] Built target kenlm_benchmark\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n","[ 93%] Built target filter\n","[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n","[ 95%] Built target kenlm_builder\n","[ 96%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n","[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n","[ 98%] Built target lmplz\n","[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n","[100%] Built target count_ngrams\n"]}]},{"cell_type":"code","source":["# Get Native Client for Scorer (Colab image is currently Ubuntu x64)\n","%cd /content/STT/data/lm\n","!wget https://github.com/coqui-ai/STT/releases/download/v{COQUI}/native_client.tflite.Linux.tar.xz\n","!tar -xJvf native_client.tflite.Linux.tar.xz\n","# fix for https://github.com/coqui-ai/STT/pull/2029/files\n","!cp /content/STT/data/lm/libkenlm.so /usr/lib/libkenlm.so\n","!ls -al"],"metadata":{"id":"lvw73Q2t6L4t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657100481451,"user_tz":-180,"elapsed":914,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"bc794bb8-04f8-4794-8c9f-14e4f00a6d52"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/STT/data/lm\n","--2022-07-06 09:41:19--  https://github.com/coqui-ai/STT/releases/download/v1.3.0/native_client.tflite.Linux.tar.xz\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/344354127/b635c5e9-a618-47a6-a952-c6427d245062?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220706T094119Z&X-Amz-Expires=300&X-Amz-Signature=fcc4441b54732e36e1b8ab7cfaec167642494632924beb282da5a91f58965efc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=344354127&response-content-disposition=attachment%3B%20filename%3Dnative_client.tflite.Linux.tar.xz&response-content-type=application%2Foctet-stream [following]\n","--2022-07-06 09:41:19--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/344354127/b635c5e9-a618-47a6-a952-c6427d245062?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220706T094119Z&X-Amz-Expires=300&X-Amz-Signature=fcc4441b54732e36e1b8ab7cfaec167642494632924beb282da5a91f58965efc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=344354127&response-content-disposition=attachment%3B%20filename%3Dnative_client.tflite.Linux.tar.xz&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3219180 (3.1M) [application/octet-stream]\n","Saving to: ‘native_client.tflite.Linux.tar.xz’\n","\n","native_client.tflit 100%[===================>]   3.07M  --.-KB/s    in 0.02s   \n","\n","2022-07-06 09:41:19 (201 MB/s) - ‘native_client.tflite.Linux.tar.xz’ saved [3219180/3219180]\n","\n","libstt.so\n","libkenlm.so\n","libsox.so.3\n","generate_scorer_package\n","LICENSE\n","stt\n","coqui-stt.h\n","KenLM_License_Info.txt\n","total 18504\n","drwxr-xr-x 2 root root     4096 Jul  6 09:41 .\n","drwxr-xr-x 5 root root     4096 Jul  6 09:38 ..\n","-rw-r--r-- 1 root root    17550 Mar  7 11:45 coqui-stt.h\n","-rw-r--r-- 1 root root     6393 Jul  6 09:38 generate_lm.py\n","-r-xr-xr-x 1 root root  2231592 Mar  7 11:48 generate_scorer_package\n","-rw-r--r-- 1 root root     4507 Mar  7 11:45 KenLM_License_Info.txt\n","-r-xr-xr-x 1 root root   532624 Mar  7 11:47 libkenlm.so\n","-rw-r--r-- 1 root root  2121856 Mar  7 11:46 libsox.so.3\n","-r-xr-xr-x 1 root root 10694968 Mar  7 11:48 libstt.so\n","-rw-r--r-- 1 root root    16725 Mar  7 11:45 LICENSE\n","-rw-r--r-- 1 root root  3219180 Mar  7 11:57 native_client.tflite.Linux.tar.xz\n","-rwxr-xr-x 1 root root    69192 Mar  7 11:48 stt\n"]}]},{"cell_type":"code","source":["# Check Tensorflow version and GPU availibility\n","import tensorflow as tf\n","print([tf.__version__, tf.test.is_gpu_available()])"],"metadata":{"id":"_V2Y0JAp5iv1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657100483198,"user_tz":-180,"elapsed":1748,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"af26799e-dccf-459e-e834-89263eb3c83d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['1.15.4', False]\n"]}]},{"cell_type":"code","source":["# Get more detailed CPU/GPU info\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()"],"metadata":{"id":"8nnR6EwR5m2A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657100483199,"user_tz":-180,"elapsed":4,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"21468b14-dc60-48bd-d832-4a319a29844b"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 5534542053953408386, name: \"/device:XLA_CPU:0\"\n"," device_type: \"XLA_CPU\"\n"," memory_limit: 17179869184\n"," locality {\n"," }\n"," incarnation: 7641230085753301768\n"," physical_device_desc: \"device: XLA_CPU device\"]"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"Jj47NfPTrtmP"},"source":["## Directory Structure"]},{"cell_type":"code","metadata":{"id":"JVNoPhpZWOOh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657100483478,"user_tz":-180,"elapsed":282,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"1ac56c5f-0833-4e6a-9d97-883b3c615b0b"},"source":["# Copy corpus data from drive\n","!mkdir -p {LOCALPATH}\n","!ls -al {LOCALPATH}"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["total 8\n","drwxr-xr-x 2 root root 4096 Jul  6 09:41 .\n","drwxr-xr-x 3 root root 4096 Jul  6 09:41 ..\n"]}]},{"cell_type":"markdown","metadata":{"id":"cCDUJlnIldHK"},"source":["## Generate Language Model"]},{"cell_type":"code","source":["# See your options\n","!python3 generate_lm.py --help"],"metadata":{"id":"MRlIkKXI6zM3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657100483740,"user_tz":-180,"elapsed":264,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"42ca50f5-39d1-4410-f1ae-76c056079f52"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: generate_lm.py [-h] --input_txt INPUT_TXT --output_dir OUTPUT_DIR\n","                      --top_k TOP_K --kenlm_bins KENLM_BINS --arpa_order\n","                      ARPA_ORDER --max_arpa_memory MAX_ARPA_MEMORY\n","                      --arpa_prune ARPA_PRUNE --binary_a_bits BINARY_A_BITS\n","                      --binary_q_bits BINARY_Q_BITS --binary_type BINARY_TYPE\n","                      [--discount_fallback]\n","\n","Generate lm.binary and top-k vocab for Coqui STT.\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --input_txt INPUT_TXT\n","                        Path to a file.txt or file.txt.gz with sample\n","                        sentences\n","  --output_dir OUTPUT_DIR\n","                        Directory path for the output\n","  --top_k TOP_K         Use top_k most frequent words for the vocab.txt file.\n","                        These will be used to filter the ARPA file.\n","  --kenlm_bins KENLM_BINS\n","                        File path to the KENLM binaries lmplz, filter and\n","                        build_binary\n","  --arpa_order ARPA_ORDER\n","                        Order of k-grams in ARPA-file generation\n","  --max_arpa_memory MAX_ARPA_MEMORY\n","                        Maximum allowed memory usage for ARPA-file generation\n","  --arpa_prune ARPA_PRUNE\n","                        ARPA pruning parameters. Separate values with '|'\n","  --binary_a_bits BINARY_A_BITS\n","                        Build binary quantization value a in bits\n","  --binary_q_bits BINARY_Q_BITS\n","                        Build binary quantization value q in bits\n","  --binary_type BINARY_TYPE\n","                        Build binary data structure type\n","  --discount_fallback   To try when such message is returned by kenlm: 'Could\n","                        not calculate Kneser-Ney discounts [...] rerun with\n","                        --discount_fallback'\n"]}]},{"cell_type":"code","source":["# Generate\n","!python3 ./generate_lm.py \\\n","  --input_txt {DRIVEPATH}/{TEXTDIR}/{LANGUAGECODE}.txt \\\n","  --output_dir {LOCALPATH}/ \\\n","  --top_k 1000 \\\n","  --discount_fallback \\\n","  --kenlm_bins /content/kenlm/build/bin/ \\\n","  --arpa_order 4 \\\n","  --arpa_prune \"0\" \\\n","  --max_arpa_memory \"85%\" \\\n","  --binary_a_bits 255 \\\n","  --binary_q_bits 8 \\\n","  --binary_type trie\n"],"metadata":{"id":"jEvpviNKweOO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657101552389,"user_tz":-180,"elapsed":5867,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"c13c1ee8-2b83-4d88-d679-afcf7845dbe0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Converting to lowercase and counting word occurrences ...\n","| | #                                              | 15979 Elapsed Time: 0:00:00\n","\n","Saving top 1000 words ...\n","\n","Calculating word statistics ...\n","  Your text file has 53992 words in total\n","  It has 387 unique words\n","  Your top-1000 words are 100.0000 percent of all words\n","  Your most common word \"vezir\" occurred 4882 times\n","  The least common word in your top-k is \"şahmat\" with 1 times\n","  The first word with 2 occurrences is \"vezire\" at place 361\n","\n","Creating ARPA file ...\n","=== 1/5 Counting and sorting n-grams ===\n","Reading /content/data/lm/lower.txt.gz\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","tcmalloc: large alloc 2309988352 bytes == 0x56477eaf2000 @  0x7fe2abe121e7 0x56477c6bf912 0x56477c65a62e 0x56477c63941b 0x56477c625176 0x7fe2a9fabc87 0x56477c626cda\n","tcmalloc: large alloc 9239928832 bytes == 0x5648085ec000 @  0x7fe2abe121e7 0x56477c6bf912 0x56477c6ae93a 0x56477c6af378 0x56477c639438 0x56477c625176 0x7fe2a9fabc87 0x56477c626cda\n","****************************************************************************************************\n","Unigram tokens 53992 types 390\n","=== 2/5 Calculating and sorting adjusted counts ===\n","Chain sizes: 1:4680 2:1970225152 3:3694172160 4:5910675456\n","tcmalloc: large alloc 5910675456 bytes == 0x56477e9e4000 @  0x7fe2abe121e7 0x56477c6bf912 0x56477c6ae93a 0x56477c6af378 0x56477c639a07 0x56477c625176 0x7fe2a9fabc87 0x56477c626cda\n","tcmalloc: large alloc 1970225152 bytes == 0x5648deecc000 @  0x7fe2abe121e7 0x56477c6bf912 0x56477c6ae93a 0x56477c6af378 0x56477c639e0d 0x56477c625176 0x7fe2a9fabc87 0x56477c626cda\n","tcmalloc: large alloc 3694174208 bytes == 0x564a2fa62000 @  0x7fe2abe121e7 0x56477c6bf912 0x56477c6ae93a 0x56477c6af378 0x56477c639e0d 0x56477c625176 0x7fe2a9fabc87 0x56477c626cda\n","Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n","Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n","Statistics:\n","1 390 D1=0.5 D2=1 D3+=1.5\n","2 5524 D1=0.5 D2=1 D3+=1.5\n","3 17611 D1=0.812081 D2=0.380058 D3+=1.91828\n","4 27368 D1=0.659237 D2=1.85675 D3+=2.20622\n","Memory estimate for binary LM:\n","type      kB\n","probing 1033 assuming -p 1.5\n","probing 1170 assuming -r models -p 1.5\n","trie     388 without quantization\n","trie     183 assuming -q 8 -b 8 quantization \n","trie     368 assuming -a 22 array pointer compression\n","trie     163 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n","=== 3/5 Calculating and sorting initial probabilities ===\n","Chain sizes: 1:4680 2:88384 3:352220 4:656832\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 4/5 Calculating and writing order-interpolated probabilities ===\n","Chain sizes: 1:4680 2:88384 3:352220 4:656832\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 5/5 Writing ARPA model ===\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Name:lmplz\tVmPeak:15256052 kB\tVmRSS:2281144 kB\tRSSMax:2300104 kB\tuser:0.283385\tsys:3.59375\tCPU:3.87719\treal:3.88088\n","\n","Filtering ARPA file using vocabulary of top-k words ...\n","Reading /content/data/lm/lm.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","\n","Building lm.binary ...\n","Reading /content/data/lm/lm_filtered.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Identifying n-grams omitted by SRI\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Quantizing\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Writing trie\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","SUCCESS\n"]}]},{"cell_type":"markdown","metadata":{"id":"W2GQsCcq678l"},"source":["## Generate Scorer"]},{"cell_type":"code","source":["# See your options\n","!./generate_scorer_package --help"],"metadata":{"id":"zhEVSKh_SD4t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657101558383,"user_tz":-180,"elapsed":272,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"0b59eb6a-e360-4774-d8ca-4064d7cbc003"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Options:\n","  --help                        show help message\n","  --checkpoint arg              Path to a checkpoint directory corresponding to\n","                                the model this scorer will be used with. The \n","                                alphabet will be loaded from an alphabet.txt \n","                                file in the checkpoint directory. Words with \n","                                characters not in the alphabet will not be \n","                                included in the vocabulary. Optional if using \n","                                bytes output mode.\n","  --lm arg                      Path of KenLM binary LM file. Must be built \n","                                without including the vocabulary (use the -v \n","                                flag). See generate_lm.py for how to create a \n","                                binary LM.\n","  --vocab arg                   Path of vocabulary file. Must contain words \n","                                separated by whitespace.\n","  --package arg                 Path to save scorer package.\n","  --default_alpha arg           Default value of alpha hyperparameter (float).\n","  --default_beta arg            Default value of beta hyperparameter (float).\n","  --force_bytes_output_mode arg Boolean flag, force set or unset bytes output \n","                                mode in the scorer package. If not set, infers \n","                                from the vocabulary. See \n","                                <https://stt.readthedocs.io/en/latest/Decoder.h\n","                                tml#bytes-output-mode> for further explanation.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"rX7APTpZ7QWs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657101561734,"user_tz":-180,"elapsed":956,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"3ddb8126-c237-4310-84dd-6faacea97e27"},"source":["# Generate scorer with somewhat arbitrary values\n","# API Change (2022-02): --alphabet => --checkpoint\n","!./generate_scorer_package \\\n","  --checkpoint {DRIVEPATH}/{CHECKPOINSTDIR}/{LANGUAGECODE} \\\n","  --lm {LOCALPATH}/lm.binary \\\n","  --vocab {LOCALPATH}/vocab-1000.txt \\\n","  --package {LOCALPATH}/kenlm.scorer \\\n","  --default_alpha 0.931289039105002 \\\n","  --default_beta 1.1834137581510284"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["387 unique words read from vocabulary file.\n","Doesn't look like a character based (Bytes Are All You Need) model.\n","--force_bytes_output_mode was not specified, using value infered from vocabulary contents: false\n","Package created in /content/data/lm/kenlm.scorer.\n"]}]},{"cell_type":"markdown","metadata":{"id":"QOxZnGbfldHL"},"source":["## Save Intermediate Results"]},{"cell_type":"code","metadata":{"id":"2j9QZb0VldHL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657101566480,"user_tz":-180,"elapsed":288,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"a07910cb-3a5b-41b7-f4b0-27c6608971be"},"source":["!ls -al {LOCALPATH}"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["total 360\n","drwxr-xr-x 2 root root   4096 Jul  6 09:59 .\n","drwxr-xr-x 3 root root   4096 Jul  6 09:41 ..\n","-rw-r--r-- 1 root root 180496 Jul  6 09:59 kenlm.scorer\n","-rw-r--r-- 1 root root 167761 Jul  6 09:59 lm.binary\n","-rw-r--r-- 1 root root   4622 Jul  6 09:59 vocab-1000.txt\n"]}]},{"cell_type":"code","metadata":{"id":"eaDeh672E5J2","executionInfo":{"status":"ok","timestamp":1657101569945,"user_tz":-180,"elapsed":1072,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}}},"source":["# Copy to drive while renaming\n","!cp {LOCALPATH}/lm.binary {DRIVEPATH}/{LMDIR}/{LANGUAGECODE}.binary\n","!cp {LOCALPATH}/vocab-1000.txt {DRIVEPATH}/{LMDIR}/{LANGUAGECODE}-vocab.txt\n","!cp {LOCALPATH}/kenlm.scorer {DRIVEPATH}/{LMDIR}/{LANGUAGECODE}.scorer"],"execution_count":19,"outputs":[]},{"cell_type":"code","source":["!ls -al {DRIVEPATH}/{LMDIR}/{LANGUAGECODE}*.*"],"metadata":{"id":"ZC9Uu5DF6oiH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657101571601,"user_tz":-180,"elapsed":395,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"9a7b6761-ef94-458d-e577-caeb9792403b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["-rw------- 1 root root 167761 Jul  6 09:59 /content/drive/MyDrive/voice-chess/lm/tr.binary\n","-rw------- 1 root root 180496 Jul  6 09:59 /content/drive/MyDrive/voice-chess/lm/tr.scorer\n","-rw------- 1 root root   4622 Jul  6 09:59 /content/drive/MyDrive/voice-chess/lm/tr-vocab.txt\n"]}]},{"cell_type":"code","metadata":{"id":"BpA5FDBYL-uO","executionInfo":{"status":"ok","timestamp":1657101577952,"user_tz":-180,"elapsed":4767,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}}},"source":["# Flush disk to Google Drive\n","drive.flush_and_unmount()"],"execution_count":21,"outputs":[]}]}