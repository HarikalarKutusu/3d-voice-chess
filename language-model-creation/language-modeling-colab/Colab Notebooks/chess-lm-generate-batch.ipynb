{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chess-lm-generate-batch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PX19Ree5UBZP"},"source":["# Create Multiple Language Models + Scorers for Voice-Chess\n","\n","Coqui v1.3.0 + KenLM latest / 4-gram NO-PRUNING!\n","\n","This notebook does not require GPU.\n","\n","Default directory structure on Google Drive (pre-generated):\n","```\n","/voice-chess        # application\n","  /lm-raw-text      # raw command text generated\n","  /checkpoints      # checkpoints for languages (produced by previous trainings)\n","    /en             # each in own directory\n","    /tr\n","    ...\n","  /lm               # Language models and scorers generated\n","  /models           # Acoustic models collected (not used here, but needed for inference in application)\n","```\n","\n","**INPUT:**\n","\n","1. Put your localized raw chess commands in /voice-chess/lm-raw-text directory as <languagecode>.txt file (e.g. en.txt)\n","2. You alphabet for the model you will use should be under /voice-chess/checkpoints/<languagecode> directory (e.g. /voice-chess/checkpoints/en/* ).\n","\n","**OUTPUT:**\n","\n","Result files can be found in /voice-chess/lm directory. Existing files will be overwritten. From these, *.scorer file will be used in the application.\n","*.txt files will include all tokens\n","*.binary file is your language model"]},{"cell_type":"markdown","source":["<H2>SPECIFY LANGUAGE CODES</H2>"],"metadata":{"id":"dDT06U33nmlP"}},{"cell_type":"code","source":["LANGUAGECODES = ['de', 'en', 'tr']"],"metadata":{"id":"wXyZijgcnviT","executionInfo":{"status":"ok","timestamp":1657102196647,"user_tz":-180,"elapsed":278,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Other Constants, adapt if needed\n","COQUI=\"1.3.0\"                                     # Coqui STT version used\n","DRIVEPATH=\"/content/drive/MyDrive/voice-chess\"    # Where you keep your work on Google Drive\n","LOCALPATH=\"/content/data/lm\"                      # A local working directory in Colab\n","TEXTDIR=\"lm-raw-text\"                             # Subdirectory names\n","LMDIR=\"lm\"\n","CHECKPOINSTDIR=\"checkpoints\""],"metadata":{"id":"S2kdGoFy28IB","executionInfo":{"status":"ok","timestamp":1657102197788,"user_tz":-180,"elapsed":2,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fMRwVKFCJmtm"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"L6MGhhWLB56b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102206794,"user_tz":-180,"elapsed":281,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"53bfe813-4d0b-41e4-a8ec-7ccaaf21ed77"},"source":["# Switch back to v1 - See: https://colab.research.google.com/notebooks/tensorflow_version.ipynb#scrollTo=NeWVBhf1VxlH\n","%tensorflow_version 1.x"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"id":"V4b9iJI7JZ6e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102225284,"user_tz":-180,"elapsed":16819,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"f2fcd7b8-def9-4682-9263-3f2ad74faf40"},"source":["# mount your private google drive\n","from google.colab import drive\n","import shutil\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"NAMwVOvtJvZj"},"source":["## Basic Setup"]},{"cell_type":"code","metadata":{"id":"URSGwY5qr3gQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102298776,"user_tz":-180,"elapsed":73497,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"5d5a9a8c-e0bd-4e04-84b0-9b29e67cde5c"},"source":["# Install Coqui STT \n","!git clone --depth 1 --branch v{COQUI} https://github.com/coqui-ai/STT.git\n","!cd STT; pip install -U pip wheel setuptools; pip install ."],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'STT'...\n","remote: Enumerating objects: 2202, done.\u001b[K\n","remote: Counting objects: 100% (2202/2202), done.\u001b[K\n","remote: Compressing objects: 100% (1398/1398), done.\u001b[K\n","remote: Total 2202 (delta 850), reused 1630 (delta 714), pack-reused 0\u001b[K\n","Receiving objects: 100% (2202/2202), 12.86 MiB | 5.65 MiB/s, done.\n","Resolving deltas: 100% (850/850), done.\n","Note: checking out '148fa74387a2082555dabd243193c6ca8cb19016'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by performing another checkout.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -b with the checkout command again. Example:\n","\n","  git checkout -b <new-branch-name>\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 9.4 MB/s \n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n","Collecting setuptools\n","  Downloading setuptools-63.1.0-py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 43.4 MB/s \n","\u001b[?25hInstalling collected packages: setuptools, pip\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed pip-22.1.2 setuptools-63.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/STT\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting attrdict\n","  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (0.0.1)\n","Collecting coqpit\n","  Downloading coqpit-0.0.16-py3-none-any.whl (13 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (1.21.6)\n","Collecting optuna\n","  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numba<=0.53.1 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (0.51.2)\n","Collecting opuslib==2.0.0\n","  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (1.3.5)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (3.38.0)\n","Collecting pyogg>=0.6.14a1\n","  Downloading PyOgg-0.6.14a1.tar.gz (35 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (0.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (2.23.0)\n","Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (2.13.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (1.15.0)\n","Collecting sox\n","  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (0.10.3.post1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.3.0) (4.64.0)\n","Collecting webdataset\n","  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting miniaudio\n","  Downloading miniaudio-1.51-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.5/588.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coqui_stt_ctcdecoder==1.3.0\n","  Downloading coqui_stt_ctcdecoder-1.3.0-cp37-cp37m-manylinux_2_24_x86_64.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow==1.15.4\n","  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (0.37.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.14.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.1.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.0.8)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (3.17.3)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (0.8.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (3.3.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.46.3)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.15.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.1.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.1.0)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.15.0)\n","Collecting numpy\n","  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui-stt-training==1.3.0) (63.1.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui-stt-training==1.3.0) (0.34.0)\n","Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->coqui-stt-training==1.3.0) (1.4.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->coqui-stt-training==1.3.0) (4.6.3)\n","Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from miniaudio->coqui-stt-training==1.3.0) (1.15.0)\n","Collecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna->coqui-stt-training==1.3.0) (3.13)\n","Collecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Collecting alembic\n","  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.3/209.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui-stt-training==1.3.0) (21.3)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui-stt-training==1.3.0) (1.4.37)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui-stt-training==1.3.0) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui-stt-training==1.3.0) (2.8.2)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->coqui-stt-training==1.3.0) (3.3.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.3.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.3.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.3.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.3.0) (2022.6.15)\n","Collecting braceexpand\n","  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12.0->miniaudio->coqui-stt-training==1.3.0) (2.21)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->coqui-stt-training==1.3.0) (3.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna->coqui-stt-training==1.3.0) (3.0.9)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->coqui-stt-training==1.3.0) (4.11.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /tensorflow-1.15.2/python3.7 (from sqlalchemy>=1.1.0->optuna->coqui-stt-training==1.3.0) (0.4.15)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui-stt-training==1.3.0) (3.3.7)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.0.1)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->coqui-stt-training==1.3.0) (5.7.1)\n","Collecting Mako\n","  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui-stt-training==1.3.0) (3.3.0)\n","Collecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui-stt-training==1.3.0) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui-stt-training==1.3.0) (4.1.1)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui-stt-training==1.3.0) (21.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna->coqui-stt-training==1.3.0) (3.8.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.4->coqui-stt-training==1.3.0) (1.5.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna->coqui-stt-training==1.3.0) (2.0.1)\n","Building wheels for collected packages: coqui-stt-training, opuslib, gast, pyogg, pyperclip\n","  Building wheel for coqui-stt-training (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for coqui-stt-training: filename=coqui_stt_training-1.3.0-py3-none-any.whl size=87226 sha256=d6b0fc850f8f7cac72a82e2abef23e9e7e916b64d6133f7234b222e41105ad1d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-4e9cmf93/wheels/59/0f/67/991d0aff52677d5260e383afececfae694d28b02bfeaacf4ed\n","  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=10988 sha256=35f99926f6651ef408a667b9dacaaef1e05235c2229e31a07cd8476f3b6d1767\n","  Stored in directory: /root/.cache/pip/wheels/e5/ba/d4/0e81231a9797fbb262ae3a54fd761fab850db7f32d94a3283a\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=edfa8db779b00d9ee51123f586db5b2dd51c484e656c0fe711e43523e937072d\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","  Building wheel for pyogg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyogg: filename=PyOgg-0.6.14a1-py2.py3-none-any.whl size=35311 sha256=254c46436a8985a5fe06f9649dbc6e7fbcc057b4ba73e6a00011828787badca6\n","  Stored in directory: /root/.cache/pip/wheels/45/aa/6f/5fb54a0a14846e4202945a65937c7f3eb245af5031a141147a\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11123 sha256=61077db429bb1ea7e5cc78dc3062a371ac6fff5b2394d8d205bcf7c211f6f721\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built coqui-stt-training opuslib gast pyogg pyperclip\n","Installing collected packages: pyperclip, pyogg, opuslib, braceexpand, pbr, numpy, gast, coqpit, colorlog, autopage, attrdict, webdataset, stevedore, sox, miniaudio, Mako, coqui_stt_ctcdecoder, cmd2, cmaes, cliff, alembic, tensorflow, optuna, coqui-stt-training\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 1.15.2\n","    Uninstalling tensorflow-1.15.2:\n","      Successfully uninstalled tensorflow-1.15.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lucid 0.3.10 requires umap-learn, which is not installed.\n","xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.4 which is incompatible.\n","jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.8 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Mako-1.2.1 alembic-1.8.0 attrdict-2.0.1 autopage-0.5.1 braceexpand-0.1.7 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 coqpit-0.0.16 coqui-stt-training-1.3.0 coqui_stt_ctcdecoder-1.3.0 gast-0.2.2 miniaudio-1.51 numpy-1.18.5 optuna-2.10.1 opuslib-2.0.0 pbr-5.9.0 pyogg-0.6.14a1 pyperclip-1.8.2 sox-1.4.1 stevedore-3.5.0 tensorflow-2.8.2+zzzcolab20220527125636 webdataset-0.2.5\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# Tensorflow GPU\n","# Needed if you want to run evaluate to test / voice & text corpus needed for that, so we leave it out\n","#!pip install tensorflow-gpu==1.15.4"],"metadata":{"id":"K4wBlgf25QyN","executionInfo":{"status":"ok","timestamp":1657102298776,"user_tz":-180,"elapsed":12,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"FLp19SjmFBLj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102402680,"user_tz":-180,"elapsed":103915,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"d2a329dc-3037-42a2-d173-f0273f7b899a"},"source":["# Get KenLM\n","!git clone https://github.com/kpu/kenlm.git && cd kenlm && mkdir build && cd build/ && cmake .. && make -j 4"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'kenlm'...\n","remote: Enumerating objects: 14102, done.\u001b[K\n","remote: Counting objects: 100% (415/415), done.\u001b[K\n","remote: Compressing objects: 100% (290/290), done.\u001b[K\n","remote: Total 14102 (delta 126), reused 371 (delta 111), pack-reused 13687\u001b[K\n","Receiving objects: 100% (14102/14102), 5.89 MiB | 16.01 MiB/s, done.\n","Resolving deltas: 100% (8006/8006), done.\n","-- The C compiler identification is GNU 7.5.0\n","-- The CXX compiler identification is GNU 7.5.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n","-- Looking for pthread.h\n","-- Looking for pthread.h - found\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n","-- Looking for pthread_create in pthreads\n","-- Looking for pthread_create in pthreads - not found\n","-- Looking for pthread_create in pthread\n","-- Looking for pthread_create in pthread - found\n","-- Found Threads: TRUE  \n","-- Found Boost: /usr/include (found suitable version \"1.65.1\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework chrono date_time atomic \n","-- Check if compiler accepts -pthread\n","-- Check if compiler accepts -pthread - yes\n","-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n","-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n","-- Looking for BZ2_bzCompressInit\n","-- Looking for BZ2_bzCompressInit - found\n","-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n","-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n","-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n","-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n","-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n","-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n","-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.2\") \n","-- Looking for clock_gettime in rt\n","-- Looking for clock_gettime in rt - found\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/kenlm/build\n","[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n","[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n","[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n","[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-to-string.cc.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/string-to-double.cc.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n","[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n","[ 38%] Built target kenlm_util\n","[ 40%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n","[ 53%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n","[ 53%] Built target kenlm_filter\n","[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n","[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n","[ 62%] Built target probing_hash_table_benchmark\n","[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n","[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n","[ 71%] Built target kenlm\n","[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n","[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n","[ 78%] Built target fragment\n","[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n","[ 80%] Built target build_binary\n","[ 81%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n","[ 82%] Built target query\n","[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n","[ 86%] Built target phrase_table_vocab\n","[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n","[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n","[ 93%] Built target filter\n","[ 93%] Built target kenlm_benchmark\n","[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n","[ 95%] Built target kenlm_builder\n","[ 96%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n","[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n","[ 98%] Built target lmplz\n","[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n","[100%] Built target count_ngrams\n"]}]},{"cell_type":"code","source":["# Get Native Client for Scorer (Colab image is currently Ubuntu x64)\n","%cd /content/STT/data/lm\n","!wget https://github.com/coqui-ai/STT/releases/download/v{COQUI}/native_client.tflite.Linux.tar.xz\n","!tar -xJvf native_client.tflite.Linux.tar.xz\n","# fix for https://github.com/coqui-ai/STT/pull/2029/files\n","!cp /content/STT/data/lm/libkenlm.so /usr/lib/libkenlm.so\n","!ls -al"],"metadata":{"id":"lvw73Q2t6L4t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102403986,"user_tz":-180,"elapsed":1322,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"6eaa7680-ce5c-400d-bb2c-98dea12b527f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/STT/data/lm\n","--2022-07-06 10:13:21--  https://github.com/coqui-ai/STT/releases/download/v1.3.0/native_client.tflite.Linux.tar.xz\n","Resolving github.com (github.com)... 192.30.255.113\n","Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/344354127/b635c5e9-a618-47a6-a952-c6427d245062?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220706T101321Z&X-Amz-Expires=300&X-Amz-Signature=34e40af27dbfb6e84868ae20cf7948a9734bbf17d9e1adc82abc8b7bdd439b6e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=344354127&response-content-disposition=attachment%3B%20filename%3Dnative_client.tflite.Linux.tar.xz&response-content-type=application%2Foctet-stream [following]\n","--2022-07-06 10:13:21--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/344354127/b635c5e9-a618-47a6-a952-c6427d245062?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220706T101321Z&X-Amz-Expires=300&X-Amz-Signature=34e40af27dbfb6e84868ae20cf7948a9734bbf17d9e1adc82abc8b7bdd439b6e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=344354127&response-content-disposition=attachment%3B%20filename%3Dnative_client.tflite.Linux.tar.xz&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3219180 (3.1M) [application/octet-stream]\n","Saving to: ‘native_client.tflite.Linux.tar.xz’\n","\n","native_client.tflit 100%[===================>]   3.07M  --.-KB/s    in 0.07s   \n","\n","2022-07-06 10:13:21 (43.5 MB/s) - ‘native_client.tflite.Linux.tar.xz’ saved [3219180/3219180]\n","\n","libstt.so\n","libkenlm.so\n","libsox.so.3\n","generate_scorer_package\n","LICENSE\n","stt\n","coqui-stt.h\n","KenLM_License_Info.txt\n","total 18504\n","drwxr-xr-x 2 root root     4096 Jul  6 10:13 .\n","drwxr-xr-x 5 root root     4096 Jul  6 10:10 ..\n","-rw-r--r-- 1 root root    17550 Mar  7 11:45 coqui-stt.h\n","-rw-r--r-- 1 root root     6393 Jul  6 10:10 generate_lm.py\n","-r-xr-xr-x 1 root root  2231592 Mar  7 11:48 generate_scorer_package\n","-rw-r--r-- 1 root root     4507 Mar  7 11:45 KenLM_License_Info.txt\n","-r-xr-xr-x 1 root root   532624 Mar  7 11:47 libkenlm.so\n","-rw-r--r-- 1 root root  2121856 Mar  7 11:46 libsox.so.3\n","-r-xr-xr-x 1 root root 10694968 Mar  7 11:48 libstt.so\n","-rw-r--r-- 1 root root    16725 Mar  7 11:45 LICENSE\n","-rw-r--r-- 1 root root  3219180 Mar  7 11:57 native_client.tflite.Linux.tar.xz\n","-rwxr-xr-x 1 root root    69192 Mar  7 11:48 stt\n"]}]},{"cell_type":"code","source":["# Check Tensorflow version and GPU availibility\n","import tensorflow as tf\n","print([tf.__version__, tf.test.is_gpu_available()])"],"metadata":{"id":"_V2Y0JAp5iv1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102406654,"user_tz":-180,"elapsed":2671,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"12de59a9-85e4-4ee8-edf0-1515045f33f6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['1.15.4', False]\n"]}]},{"cell_type":"code","source":["# Get more detailed CPU/GPU info\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()"],"metadata":{"id":"8nnR6EwR5m2A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102406655,"user_tz":-180,"elapsed":5,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"bed98eb3-bd0e-4299-9152-2d19781645b3"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 10617295124985941042, name: \"/device:XLA_CPU:0\"\n"," device_type: \"XLA_CPU\"\n"," memory_limit: 17179869184\n"," locality {\n"," }\n"," incarnation: 5554728172758096310\n"," physical_device_desc: \"device: XLA_CPU device\"]"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"Jj47NfPTrtmP"},"source":["## Directory Structure"]},{"cell_type":"code","metadata":{"id":"JVNoPhpZWOOh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102406942,"user_tz":-180,"elapsed":290,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"46eeba43-c3b6-45df-f18a-0a47e392f5ec"},"source":["# Copy corpus data from drive\n","!mkdir -p {LOCALPATH}\n","!ls -al {LOCALPATH}"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["total 8\n","drwxr-xr-x 2 root root 4096 Jul  6 10:13 .\n","drwxr-xr-x 3 root root 4096 Jul  6 10:13 ..\n"]}]},{"cell_type":"markdown","metadata":{"id":"cCDUJlnIldHK"},"source":["## Command Line Arguments"]},{"cell_type":"code","source":["# See your options\n","!python3 generate_lm.py --help"],"metadata":{"id":"MRlIkKXI6zM3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102407229,"user_tz":-180,"elapsed":289,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"7bdc11d4-af25-494d-c51c-9c79c8718c08"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: generate_lm.py [-h] --input_txt INPUT_TXT --output_dir OUTPUT_DIR\n","                      --top_k TOP_K --kenlm_bins KENLM_BINS --arpa_order\n","                      ARPA_ORDER --max_arpa_memory MAX_ARPA_MEMORY\n","                      --arpa_prune ARPA_PRUNE --binary_a_bits BINARY_A_BITS\n","                      --binary_q_bits BINARY_Q_BITS --binary_type BINARY_TYPE\n","                      [--discount_fallback]\n","\n","Generate lm.binary and top-k vocab for Coqui STT.\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --input_txt INPUT_TXT\n","                        Path to a file.txt or file.txt.gz with sample\n","                        sentences\n","  --output_dir OUTPUT_DIR\n","                        Directory path for the output\n","  --top_k TOP_K         Use top_k most frequent words for the vocab.txt file.\n","                        These will be used to filter the ARPA file.\n","  --kenlm_bins KENLM_BINS\n","                        File path to the KENLM binaries lmplz, filter and\n","                        build_binary\n","  --arpa_order ARPA_ORDER\n","                        Order of k-grams in ARPA-file generation\n","  --max_arpa_memory MAX_ARPA_MEMORY\n","                        Maximum allowed memory usage for ARPA-file generation\n","  --arpa_prune ARPA_PRUNE\n","                        ARPA pruning parameters. Separate values with '|'\n","  --binary_a_bits BINARY_A_BITS\n","                        Build binary quantization value a in bits\n","  --binary_q_bits BINARY_Q_BITS\n","                        Build binary quantization value q in bits\n","  --binary_type BINARY_TYPE\n","                        Build binary data structure type\n","  --discount_fallback   To try when such message is returned by kenlm: 'Could\n","                        not calculate Kneser-Ney discounts [...] rerun with\n","                        --discount_fallback'\n"]}]},{"cell_type":"code","source":["# See your options\n","!./generate_scorer_package --help"],"metadata":{"id":"zhEVSKh_SD4t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102407518,"user_tz":-180,"elapsed":290,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"30556527-1e71-4cc3-d6e1-9549cdb13462"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Options:\n","  --help                        show help message\n","  --checkpoint arg              Path to a checkpoint directory corresponding to\n","                                the model this scorer will be used with. The \n","                                alphabet will be loaded from an alphabet.txt \n","                                file in the checkpoint directory. Words with \n","                                characters not in the alphabet will not be \n","                                included in the vocabulary. Optional if using \n","                                bytes output mode.\n","  --lm arg                      Path of KenLM binary LM file. Must be built \n","                                without including the vocabulary (use the -v \n","                                flag). See generate_lm.py for how to create a \n","                                binary LM.\n","  --vocab arg                   Path of vocabulary file. Must contain words \n","                                separated by whitespace.\n","  --package arg                 Path to save scorer package.\n","  --default_alpha arg           Default value of alpha hyperparameter (float).\n","  --default_beta arg            Default value of beta hyperparameter (float).\n","  --force_bytes_output_mode arg Boolean flag, force set or unset bytes output \n","                                mode in the scorer package. If not set, infers \n","                                from the vocabulary. See \n","                                <https://stt.readthedocs.io/en/latest/Decoder.h\n","                                tml#bytes-output-mode> for further explanation.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"duI-F5AE9wzK"},"source":["## Generate Language Models in a Loop"]},{"cell_type":"code","source":["for lc in LANGUAGECODES:\n","  # Generate LM\n","  !python3 ./generate_lm.py \\\n","    --input_txt {DRIVEPATH}/{TEXTDIR}/{lc}.txt \\\n","    --output_dir {LOCALPATH}/ \\\n","    --top_k 1000 \\\n","    --discount_fallback \\\n","    --kenlm_bins /content/kenlm/build/bin/ \\\n","    --arpa_order 4 \\\n","    --arpa_prune \"0\" \\\n","    --max_arpa_memory \"85%\" \\\n","    --binary_a_bits 255 \\\n","    --binary_q_bits 8 \\\n","    --binary_type trie\n","  \n","  # Generate scorer with somewhat arbitrary values\n","  # API Change (2022-02): --alphabet => --checkpoint\n","  !./generate_scorer_package \\\n","    --checkpoint {DRIVEPATH}/{CHECKPOINSTDIR}/{lc} \\\n","    --lm {LOCALPATH}/lm.binary \\\n","    --vocab {LOCALPATH}/vocab-1000.txt \\\n","    --package {LOCALPATH}/kenlm.scorer \\\n","    --default_alpha 0.931289039105002 \\\n","    --default_beta 1.1834137581510284\n","  \n","  # Copy to drive while renaming\n","  !mv {LOCALPATH}/lm.binary {DRIVEPATH}/{LMDIR}/{lc}.binary\n","  !mv {LOCALPATH}/vocab-1000.txt {DRIVEPATH}/{LMDIR}/{lc}-vocab.txt\n","  !mv {LOCALPATH}/kenlm.scorer {DRIVEPATH}/{LMDIR}/{lc}.scorer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6cxRoRa97Sv","executionInfo":{"status":"ok","timestamp":1657102425130,"user_tz":-180,"elapsed":12947,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"48129da0-fc62-4b0f-b678-9cd8e1ee5fff"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Converting to lowercase and counting word occurrences ...\n","| | #                                              | 16192 Elapsed Time: 0:00:00\n","\n","Saving top 1000 words ...\n","\n","Calculating word statistics ...\n","  Your text file has 74213 words in total\n","  It has 132 unique words\n","  Your top-1000 words are 100.0000 percent of all words\n","  Your most common word \"nach\" occurred 12382 times\n","  The least common word in your top-k is \"zuruecksetzen\" with 1 times\n","  The first word with 4 occurrences is \"zur\" at place 100\n","\n","Creating ARPA file ...\n","=== 1/5 Counting and sorting n-grams ===\n","Reading /content/data/lm/lower.txt.gz\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","tcmalloc: large alloc 2309988352 bytes == 0x55895c0fa000 @  0x7f8acd54a1e7 0x55895aec5912 0x55895ae6062e 0x55895ae3f41b 0x55895ae2b176 0x7f8acb6e3c87 0x55895ae2ccda\n","tcmalloc: large alloc 9239937024 bytes == 0x5589e5bf4000 @  0x7f8acd54a1e7 0x55895aec5912 0x55895aeb493a 0x55895aeb5378 0x55895ae3f438 0x55895ae2b176 0x7f8acb6e3c87 0x55895ae2ccda\n","****************************************************************************************************\n","Unigram tokens 74213 types 135\n","=== 2/5 Calculating and sorting adjusted counts ===\n","Chain sizes: 1:1620 2:1970226688 3:3694174976 4:5910680064\n","tcmalloc: large alloc 5910683648 bytes == 0x55895bfec000 @  0x7f8acd54a1e7 0x55895aec5912 0x55895aeb493a 0x55895aeb5378 0x55895ae3fa07 0x55895ae2b176 0x7f8acb6e3c87 0x55895ae2ccda\n","tcmalloc: large alloc 1970233344 bytes == 0x558abc4d6000 @  0x7f8acd54a1e7 0x55895aec5912 0x55895aeb493a 0x55895aeb5378 0x55895ae3fe0d 0x55895ae2b176 0x7f8acb6e3c87 0x55895ae2ccda\n","tcmalloc: large alloc 3694182400 bytes == 0x558c0d06c000 @  0x7f8acd54a1e7 0x55895aec5912 0x55895aeb493a 0x55895aeb5378 0x55895ae3fe0d 0x55895ae2b176 0x7f8acb6e3c87 0x55895ae2ccda\n","Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n","Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n","Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n","Substituting fallback discounts for order 3: D1=0.5 D2=1 D3+=1.5\n","Statistics:\n","1 135 D1=0.5 D2=1 D3+=1.5\n","2 1801 D1=0.5 D2=1 D3+=1.5\n","3 6404 D1=0.5 D2=1 D3+=1.5\n","4 15605 D1=0.5 D2=1 D3+=1.5\n","Memory estimate for binary LM:\n","type     kB\n","probing 470 assuming -p 1.5\n","probing 518 assuming -r models -p 1.5\n","trie    162 without quantization\n","trie     76 assuming -q 8 -b 8 quantization \n","trie    157 assuming -a 22 array pointer compression\n","trie     71 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n","=== 3/5 Calculating and sorting initial probabilities ===\n","Chain sizes: 1:1620 2:28816 3:128080 4:374520\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 4/5 Calculating and writing order-interpolated probabilities ===\n","Chain sizes: 1:1620 2:28816 3:128080 4:374520\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 5/5 Writing ARPA model ===\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Name:lmplz\tVmPeak:15256064 kB\tVmRSS:2280024 kB\tRSSMax:2299288 kB\tuser:0.254757\tsys:1.58398\tCPU:1.83879\treal:1.85105\n","\n","Filtering ARPA file using vocabulary of top-k words ...\n","Reading /content/data/lm/lm.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","\n","Building lm.binary ...\n","Reading /content/data/lm/lm_filtered.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Identifying n-grams omitted by SRI\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Quantizing\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Writing trie\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","SUCCESS\n","132 unique words read from vocabulary file.\n","Doesn't look like a character based (Bytes Are All You Need) model.\n","--force_bytes_output_mode was not specified, using value infered from vocabulary contents: false\n","Package created in /content/data/lm/kenlm.scorer.\n","\n","Converting to lowercase and counting word occurrences ...\n","| | #                                              | 14329 Elapsed Time: 0:00:00\n","\n","Saving top 1000 words ...\n","\n","Calculating word statistics ...\n","  Your text file has 64278 words in total\n","  It has 122 unique words\n","  Your top-1000 words are 100.0000 percent of all words\n","  Your most common word \"to\" occurred 7818 times\n","  The least common word in your top-k is \"undo\" with 1 times\n","  The first word with 4 occurrences is \"promote\" at place 97\n","\n","Creating ARPA file ...\n","=== 1/5 Counting and sorting n-grams ===\n","Reading /content/data/lm/lower.txt.gz\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","tcmalloc: large alloc 2309988352 bytes == 0x55fb48efa000 @  0x7ffa442831e7 0x55fb4793a912 0x55fb478d562e 0x55fb478b441b 0x55fb478a0176 0x7ffa4241cc87 0x55fb478a1cda\n","tcmalloc: large alloc 9239937024 bytes == 0x55fbd29f4000 @  0x7ffa442831e7 0x55fb4793a912 0x55fb4792993a 0x55fb4792a378 0x55fb478b4438 0x55fb478a0176 0x7ffa4241cc87 0x55fb478a1cda\n","****************************************************************************************************\n","Unigram tokens 64278 types 125\n","=== 2/5 Calculating and sorting adjusted counts ===\n","Chain sizes: 1:1500 2:1970226816 3:3694175232 4:5910680576\n","tcmalloc: large alloc 5910683648 bytes == 0x55fb48dec000 @  0x7ffa442831e7 0x55fb4793a912 0x55fb4792993a 0x55fb4792a378 0x55fb478b4a07 0x55fb478a0176 0x7ffa4241cc87 0x55fb478a1cda\n","tcmalloc: large alloc 1970233344 bytes == 0x55fca92d6000 @  0x7ffa442831e7 0x55fb4793a912 0x55fb4792993a 0x55fb4792a378 0x55fb478b4e0d 0x55fb478a0176 0x7ffa4241cc87 0x55fb478a1cda\n","tcmalloc: large alloc 3694182400 bytes == 0x55fdf9e6c000 @  0x7ffa442831e7 0x55fb4793a912 0x55fb4792993a 0x55fb4792a378 0x55fb478b4e0d 0x55fb478a0176 0x7ffa4241cc87 0x55fb478a1cda\n","Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n","Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n","Statistics:\n","1 125 D1=0.894737 D2=0.657895 D3+=3\n","2 1989 D1=0.5 D2=1 D3+=1.5\n","3 8061 D1=0.5 D2=1 D3+=1.5\n","4 17617 D1=0.655181 D2=1.03001 D3+=2.5929\n","Memory estimate for binary LM:\n","type     kB\n","probing 548 assuming -p 1.5\n","probing 607 assuming -r models -p 1.5\n","trie    188 without quantization\n","trie     86 assuming -q 8 -b 8 quantization \n","trie    180 assuming -a 22 array pointer compression\n","trie     78 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n","=== 3/5 Calculating and sorting initial probabilities ===\n","Chain sizes: 1:1500 2:31824 3:161220 4:422808\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 4/5 Calculating and writing order-interpolated probabilities ===\n","Chain sizes: 1:1500 2:31824 3:161220 4:422808\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 5/5 Writing ARPA model ===\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Name:lmplz\tVmPeak:15256068 kB\tVmRSS:2279772 kB\tRSSMax:2299200 kB\tuser:0.232986\tsys:1.19892\tCPU:1.43201\treal:1.44734\n","\n","Filtering ARPA file using vocabulary of top-k words ...\n","Reading /content/data/lm/lm.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","\n","Building lm.binary ...\n","Reading /content/data/lm/lm_filtered.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Identifying n-grams omitted by SRI\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Quantizing\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Writing trie\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","SUCCESS\n","122 unique words read from vocabulary file.\n","Doesn't look like a character based (Bytes Are All You Need) model.\n","--force_bytes_output_mode was not specified, using value infered from vocabulary contents: false\n","Package created in /content/data/lm/kenlm.scorer.\n","\n","Converting to lowercase and counting word occurrences ...\n","| | #                                              | 15979 Elapsed Time: 0:00:00\n","\n","Saving top 1000 words ...\n","\n","Calculating word statistics ...\n","  Your text file has 53992 words in total\n","  It has 387 unique words\n","  Your top-1000 words are 100.0000 percent of all words\n","  Your most common word \"vezir\" occurred 4882 times\n","  The least common word in your top-k is \"şahmat\" with 1 times\n","  The first word with 2 occurrences is \"vezire\" at place 361\n","\n","Creating ARPA file ...\n","=== 1/5 Counting and sorting n-grams ===\n","Reading /content/data/lm/lower.txt.gz\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","tcmalloc: large alloc 2309988352 bytes == 0x55999a650000 @  0x7fcf2f86c1e7 0x559998103912 0x55999809e62e 0x55999807d41b 0x559998069176 0x7fcf2da05c87 0x55999806acda\n","tcmalloc: large alloc 9239937024 bytes == 0x559a2414a000 @  0x7fcf2f86c1e7 0x559998103912 0x5599980f293a 0x5599980f3378 0x55999807d438 0x559998069176 0x7fcf2da05c87 0x55999806acda\n","****************************************************************************************************\n","Unigram tokens 53992 types 390\n","=== 2/5 Calculating and sorting adjusted counts ===\n","Chain sizes: 1:4680 2:1970226176 3:3694173952 4:5910678528\n","tcmalloc: large alloc 5910683648 bytes == 0x55999a542000 @  0x7fcf2f86c1e7 0x559998103912 0x5599980f293a 0x5599980f3378 0x55999807da07 0x559998069176 0x7fcf2da05c87 0x55999806acda\n","tcmalloc: large alloc 1970233344 bytes == 0x559afaa2c000 @  0x7fcf2f86c1e7 0x559998103912 0x5599980f293a 0x5599980f3378 0x55999807de0d 0x559998069176 0x7fcf2da05c87 0x55999806acda\n","tcmalloc: large alloc 3694174208 bytes == 0x559c4b5c2000 @  0x7fcf2f86c1e7 0x559998103912 0x5599980f293a 0x5599980f3378 0x55999807de0d 0x559998069176 0x7fcf2da05c87 0x55999806acda\n","Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n","Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n","Statistics:\n","1 390 D1=0.5 D2=1 D3+=1.5\n","2 5524 D1=0.5 D2=1 D3+=1.5\n","3 17611 D1=0.812081 D2=0.380058 D3+=1.91828\n","4 27368 D1=0.659237 D2=1.85675 D3+=2.20622\n","Memory estimate for binary LM:\n","type      kB\n","probing 1033 assuming -p 1.5\n","probing 1170 assuming -r models -p 1.5\n","trie     388 without quantization\n","trie     183 assuming -q 8 -b 8 quantization \n","trie     368 assuming -a 22 array pointer compression\n","trie     163 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n","=== 3/5 Calculating and sorting initial probabilities ===\n","Chain sizes: 1:4680 2:88384 3:352220 4:656832\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 4/5 Calculating and writing order-interpolated probabilities ===\n","Chain sizes: 1:4680 2:88384 3:352220 4:656832\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 5/5 Writing ARPA model ===\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Name:lmplz\tVmPeak:15256056 kB\tVmRSS:2281164 kB\tRSSMax:2299840 kB\tuser:0.27046\tsys:1.09333\tCPU:1.36383\treal:1.37058\n","\n","Filtering ARPA file using vocabulary of top-k words ...\n","Reading /content/data/lm/lm.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","\n","Building lm.binary ...\n","Reading /content/data/lm/lm_filtered.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Identifying n-grams omitted by SRI\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Quantizing\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Writing trie\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","SUCCESS\n","387 unique words read from vocabulary file.\n","Doesn't look like a character based (Bytes Are All You Need) model.\n","--force_bytes_output_mode was not specified, using value infered from vocabulary contents: false\n","Package created in /content/data/lm/kenlm.scorer.\n"]}]},{"cell_type":"markdown","metadata":{"id":"QOxZnGbfldHL"},"source":["## Finish"]},{"cell_type":"code","source":["!ls -al {DRIVEPATH}/{LMDIR}/*.*"],"metadata":{"id":"ZC9Uu5DF6oiH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657102429680,"user_tz":-180,"elapsed":283,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}},"outputId":"9d4e65a0-3717-4877-ae34-52163bf499a2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["-rw-------+ 1 root root  73005 Jul  6 10:13 /content/drive/MyDrive/voice-chess/lm/de.binary\n","-rw-------+ 1 root root  87104 Jul  6 10:13 /content/drive/MyDrive/voice-chess/lm/de.scorer\n","-rw-------+ 1 root root   1406 Jul  6 10:13 /content/drive/MyDrive/voice-chess/lm/de-vocab.txt\n","-rw-------+ 1 root root  80704 Jul  6 10:13 /content/drive/MyDrive/voice-chess/lm/en.binary\n","-rw-------+ 1 root root  89504 Jul  6 10:13 /content/drive/MyDrive/voice-chess/lm/en.scorer\n","-rw-------+ 1 root root   1114 Jul  6 10:13 /content/drive/MyDrive/voice-chess/lm/en-vocab.txt\n","-rw-------+ 1 root root 167761 Jul  6 10:13 /content/drive/MyDrive/voice-chess/lm/tr.binary\n","-rw-------+ 1 root root 180496 Jul  6 10:13 /content/drive/MyDrive/voice-chess/lm/tr.scorer\n","-rw-------+ 1 root root   4622 Jul  6 10:13 /content/drive/MyDrive/voice-chess/lm/tr-vocab.txt\n"]}]},{"cell_type":"code","metadata":{"id":"BpA5FDBYL-uO","executionInfo":{"status":"ok","timestamp":1657102435252,"user_tz":-180,"elapsed":1620,"user":{"displayName":"Bülent Özden","userId":"05508198528846180761"}}},"source":["# Flush disk to Google Drive\n","drive.flush_and_unmount()"],"execution_count":16,"outputs":[]}]}